# -*- coding: utf-8 -*-
"""Original_File_For_Model_Building_ALL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11IWZvJ7ybnzd-i1wX-au7xoQt1YJV5zA

## **Importing Libraries and Data**
"""

from google.colab import drive
drive.mount('/content/drive')

# Importing the relevant libraries

import pandas as pd
import re
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings(action='ignore')
import collections
from scipy.stats import entropy
import nltk
from nltk.corpus import words, stopwords
nltk.download('words')
nltk.download('stopwords')
!pip install langid
from langid.langid import LanguageIdentifier, model
import gensim.downloader as api
!pip install wordninja
import wordninja
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression as LR
from sklearn.svm import SVC
!pip install blosc
!pip install pickle 

from weakref import WeakValueDictionary
from gensim import models
!pip install gensim # to upgrade version
from gensim.models import Word2Vec
import gensim

import pickle
import blosc


IS_ALL_DATA = False 


import nltk
nltk.download('gutenberg')
nltk.download('genesis')
nltk.download('inaugural')
nltk.download('nps_chat')
nltk.download('webtext')
nltk.download('treebank')
nltk.download('brown')
nltk.download('averaged_perceptron_tagger')
nltk.download("stopwords")
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('words')

import nltk.corpus
from nltk.book import *
from nltk.corpus import brown

english_vocab = set(w.lower() for w in nltk.corpus.words.words())
!pip install tld
from tld import get_tld     # For Extracting TLD

!pip install embeddings  # from pypi
!pip install git+https://github.com/vzhong/embeddings.git  # from github

from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler as MMS
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

from sklearn.tree import DecisionTreeClassifier as DTC

from sklearn.linear_model import LogisticRegression as LR
from sklearn.svm import SVC



col_Feature_All =  ['DExtension',  'is_wh_adverb', 'is_wh_pronoun', 
                       'is_wh_determiner', 'is_verb_present_3rd_person_singular', 
                     'is_verb_present', 'is_verb_past_participle', 'is_verb_past_tense',
                     'is_verb_gerund', 'is_verb', 'is_interjection', 'is_infinite_marker', 
                     'is_particle', 'is_adverb_superlative','is_adverb_comparative', 
                     'is_adverb', 'is_possessive_pronoun', 
                     'is_personal_pronoun', 'is_possessive_ending',
                    'is_predeterminer', 'is_proper_noun_plural', 
                    'is_proper_noun_singular', 'is_noun_plural', 
                    'is_noun_singular','is_modal', 'is_modal',
                    'is_list_market', 'is_adjective_superlative', 
                    'is_adjective_comparative', 'is_preposition_conjunction',
                    'is_foreign_word', 'is_existential_there', 
                    'is_determiner', 'is_cardinal_digit', 
                    'is_coordinating_conjunction', 
                    'is_adjective', 'lexical_diversity', 
                    
                  'DLength', 'DAlpha', 'DHaveDigits', 'DAllDigits', 'DHaveHyphen', 
                  'DFirstDigit','DLastDigit', 'DVowelRatio', 'DConsonantRatio', 'DHyphenRatio', 
                  'DDigitRatio', 'DShannonEntropy' ,'DWordCount', 'DLanguageScore',
                  'DPCA1', 'DPCA2', 'DPCA3', 'DPCA4', 'DPCA5']


col_Features = ['DExtension', 'DLength', 'DAlpha', 'DHaveDigits', 'DAllDigits', 'DHaveHyphen', 
                  'DFirstDigit','DLastDigit', 'DVowelRatio', 'DConsonantRatio', 'DHyphenRatio', 
                'DDigitRatio', 'DShannonEntropy' ,'DWordCount', 'DLanguageScore' , 
                  'DPCA1', 'DPCA2', 'DPCA3', 'DPCA4', 'DPCA5' ]

                  


col_Feature_No_PCA =  ['DExtension',  'is_wh_adverb', 'is_wh_pronoun', 
                       'is_wh_determiner', 'is_verb_present_3rd_person_singular', 
                     'is_verb_present', 'is_verb_past_participle', 'is_verb_past_tense',
                     'is_verb_gerund', 'is_verb', 'is_interjection', 'is_infinite_marker', 
                     'is_particle', 'is_adverb_superlative','is_adverb_comparative', 
                     'is_adverb', 'is_possessive_pronoun', 
                     'is_personal_pronoun', 'is_possessive_ending',
                    'is_predeterminer', 'is_proper_noun_plural', 
                    'is_proper_noun_singular', 'is_noun_plural', 
                    'is_noun_singular','is_modal', 'is_modal',
                    'is_list_market', 'is_adjective_superlative', 
                    'is_adjective_comparative', 'is_preposition_conjunction',
                    'is_foreign_word', 'is_existential_there', 
                    'is_determiner', 'is_cardinal_digit', 
                    'is_coordinating_conjunction', 
                    'is_adjective', 'lexical_diversity', 
                    'dictionary_words_count', 'dictionary_words_present', 
                    'character_type',
                    'words_len_min',  # 
                    
                  'DLength', 'DAlpha', 'DHaveDigits', 'DAllDigits', 'DHaveHyphen', 
                  'DFirstDigit','DLastDigit', 'DVowelRatio', 'DConsonantRatio', 'DHyphenRatio', 
                  'DDigitRatio', 'DShannonEntropy' ,'DWordCount', 'DLanguageScore']

False# mounting google drive
from google.colab import drive
drive.mount('/content/drive')
#data = pd.read_csv('/content/drive/MyDrive/Domain_Name_Prediction_Final_Processing/Data/TestData/small_test_data_10K.csv')
data = pd.read_csv('/content/drive/MyDrive/Domain_Name_Prediction_Final_Processing/Data/CleanData/DomainMerged_5_underSampling.csv')
#data = pd.read_csv('/content/drive/MyDrive/Domain_Name_Prediction_Final_Processing/Data/CleanData/DomainMerged_5.csv')


#data = pd.read_csv('/content/drive/MyDrive/Domain_Name_Prediction_Final_Processing/Data/TestData/sample_test_data_diverse4.csv') # 4 lak data (250 times less)


wordToVec_Path = '/content/drive/MyDrive/Domain_Name_Prediction_Final_Processing/Data/HelpfulData/GoogleNews-vectors-negative300.bin'
df_to_save_path_jain = '/content/drive/MyDrive/Domain_Name_Prediction_Final_Processing/Data/CleanData/DF_Jain_All_1M.csv'
saved_models_path_new = '/content/drive/MyDrive/Domain_Name_Prediction_Final_Processing/Code/SavedModels/Embedding/'


# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)
wv = gensim.models.KeyedVectors.load_word2vec_format(wordToVec_Path, binary=True)

DT_JAIN_600 = saved_models_path_new + 'DT_finalized_model_Jain_600.sav'
LR_JAIN_600 = saved_models_path_new + 'LR_finalized_model_Jain_600.sav'
BAG_JAIN_600 = saved_models_path_new + 'BAG_finalized_model_Jain_600.sav'

DT_ALL_NOPACE = saved_models_path_new + 'DT_finalized_model_all_nopca.sav'
LR_ALL_NOPACE= saved_models_path_new + 'LR_finalized_model_all_nopca.sav'
BAG_ALL_NOPACE = saved_models_path_new + 'BAG_finalized_model_all_nopca.sav'


data.head()

dist = data.category.value_counts().to_frame().reset_index()
dist['% distribution'] = data.category.value_counts().values/data.shape[0] * 100
dist.columns = ['categories', 'count', '% distribution']
dist

def get_Domain_Extension (one_value):
  ext = 'unknown'
  try:
    ext = get_tld(one_value, fix_protocol=True)
  except:
    print('cant find domain extension..')

  return (ext)

def get_Domain (one_value):
  dom = one_value
  try:
    ext = get_tld(one_value, fix_protocol=True, as_object=True)
    dom  =str(ext.domain)
  except:
    print('cant find domain')
  return dom

data['DExtension'] = data['Domain'].apply(lambda x:  get_Domain_Extension(x))
data['host'] = data['Domain'].apply(lambda x:  get_Domain(x))


# Appyl lable encoding for extension
le = preprocessing.LabelEncoder()
data['DExtension'] = le.fit_transform(data['DExtension'])

data['Domain'] = data['host']     # This will get rid f extension


print(data['DExtension'])
'''
print(data['host'])
print(data['Domain'])
'''

"""There appears to be an imbalance in the data

## **Feature Extraction**
"""

# Extracting the domain name


data['DName'] = data['Domain'].apply(lambda x: x.split('.')[0].lower())

# Extracting the domain extension
#data['DExt'] = data['Domain'].apply(lambda x: x.split('.')[1])

# Counting the number of characters in the domain name
data['DLength'] = data['DName'].apply(lambda x: len(x))

# Extracting all alphabets in the domain name and counting the number of alphabets in the string
data['DAlphaChar'] = data['DName'].apply(lambda x: ''.join(re.findall('[a-zA-Z-]', x)))
data['DAlpha'] = data['DAlphaChar'].apply(lambda x: len(x))

# Are there digits in the domain name
data['DHaveDigits'] = data['DName'].apply(lambda x: 0 if len(re.findall('\d', x)) == 0 else 1)
data['DAllDigits'] = data.apply(lambda x: 1 if len(re.findall('\d', x['DName'])) == x['DLength'] else 0, axis = 1)

# Are there hyphens in the domain name
data['DHaveHyphen'] = data['DName'].apply(lambda x: 0 if len(re.findall('[-]', x)) == 0 else 1)

# Checking if the first character of the domain name is an alphabet or digit: 1 if digit, 0 if alphabet
data['DFirstDigit'] = data['DName'].apply(lambda x: 1 if x[0].isdigit() else 0)

# Checking if the last character of the domain name is an alphabet or digit: 1 if digit, 0 if alphabet
data['DLastDigit'] = data['DName'].apply(lambda x: 1 if x[-1].isdigit() else 0)

# Define UDF to compute the ratio of vowels and consonants relative to the length of alphabets in the domain name
def extract_ratio(x, arg):
  if x['DAlpha'] == 0:
    return 0
  elif arg == 'vowel':
    return len(re.findall('[aeiou]', x['DName']))/x['DLength']
  elif arg == 'cons':
    return len(re.findall('[^aeiou0-9-]', x['DName']))/x['DLength']

# Compute the ratio of vowels to total alphabets (reflects pronouncability)
data['DVowelRatio'] = data.apply(lambda x: extract_ratio(x, 'vowel'), axis = 1)

# Compute the ratio of consonants to total alphabets
data['DConsonantRatio'] = data.apply(lambda x: extract_ratio(x, 'cons'), axis = 1)

# Compute the ratio of underscore
data['DHyphenRatio'] = data.apply(lambda x: len(re.findall('[-]', x['DName']))/x['DLength'], axis = 1)

# Compute the ratio of underscore
data['DDigitRatio'] = data.apply(lambda x: len(re.findall('\d', x['DName']))/x['DLength'], axis = 1)

# define a UDF which computes the shannon entropy of any given string
def shannon_entropy(d_name):
    # count occurence of each character in the string
    xters = collections.Counter([txt for txt in d_name])
    # compute the ratio of each character relative to the length of the string
    dist = [x/sum(xters.values()) for x in xters.values()]
    # compute the entropy
    shan_entropy = entropy(dist, base=2)
 
    return shan_entropy

data['DShannonEntropy'] = data['DName'].apply(lambda x: shannon_entropy(x))

# extract all meaningful words from the domain name if any using the wordninja library
data['DWords'] = data['DAlphaChar'].apply(lambda x: x.split('-') if '-' in x and len(x) > 5 else wordninja.split(x))

# define a function to remove stopwords and single letters from a list of words
def clean_LOW(lst):
  # list of english stop words
  stops = list(set(stopwords.words('english')))
  # list of english alphabets
  alphabets = [chr(x) for x in range(ord('a'), ord('z') + 1)]
  # set of stopwords in list of words
  stops_in_lst = set(lst).intersection(set(stops))
  # set of alphabets in list of words
  remove = set(lst).intersection(set(alphabets).union(set(stops)))
  for i in remove:
    if i in lst:
      lst.remove(i)
  for i in lst:
    if len(i) <= 2:
      lst.remove(i)
  return lst

data['DWords'] = data['DWords'].apply(lambda x: clean_LOW(x))
# count number of meaningful words in the domain name
data['DWordCount'] = data['DWords'].apply(lambda x: len(x))
data['DWordCount'].max()

cnt = data[data['DWordCount'] > 5].shape[0]
print('There are %d domain names in the dataset with 5 or more meaningful words' %cnt)

# Define a function that identified the domain names and classifies them to different languages. This may be considered the language score of the domain name i.e. a
# high language score means that the domain name is recognizable as english while a low score or score of zero means the domain name is gibberish 
identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)

# define a function that identifies each word in the string and computes a language score where the language is english
def lang_score(x):
  score = 0
  if len(x) == 0:
    pass
  else:
    for i in x:
      lang_and_score = identifier.classify(i)
      if lang_and_score[0] == 'en':
        score =+ lang_and_score[1]
    score = score/len(x)    
  return score

data['DLanguageScore'] = data['DWords'].apply(lambda x: lang_score(x))

# Loading in the pre-trained google word 2 vec 

# define function to obtain the average vector for each group of words
def vectorize(x):
  vec = np.zeros(300)
  if x['DWords'] == []:
    pass
  else:
    for i in x['DWords']:
      try:
        vec =+ wv[i]
      except:
        pass
    vec = vec/x['DWordCount']
  return vec

data['DVector'] = data.apply(lambda x: vectorize(x), axis = 1)

###############################################################################
# From Madiha
###############################################################################

if(IS_ALL_DATA ==False):
  data['words'] = data['Domain'].apply(wordninja.split)


  def calculate_words_len(val):
    if val:
      return [len(word) for word in val]
    return []

  # Will it helps, if we know, the word size is big which is found?
  def calculate_words_len_min(val):
    length = 0
    if val:
      for word in val:
        lenght = length+ len(word)
    return lenght

  data['words_len'] = data['words'].apply(calculate_words_len)
  data['words_len_min'] = data['words'].apply(calculate_words_len_min)

  def check_type(val: str):
    if val.isdigit():
      return 1
    if val.isalpha():
      return 2
    return 3

  data['character_type'] = data['Domain'].apply(check_type)

  from nltk.corpus import wordnet as wn  # Takes some time

  def get_dict_words(val):
    new_list = []
    for word in val:
      if word and not word.strip(word[0]):
        continue

      temp_l = wn.synsets(word)
      if temp_l and (word in english_vocab or temp_l[0].pos()) and not word.isdigit():
        new_list.append(word)
    return new_list

  data['dict_words'] = data['words'].apply(get_dict_words)

  data['dictionary_words_count'] = data['dict_words'].apply(lambda l: len(l))  # (3 sec)
  data['dictionary_words_present']  = data['dictionary_words_count']
  data['dictionary_words_present'].values[data['dictionary_words_present'].values > 1] = 1 

  def lexical_diversity(text):
      dum = 0
      try:
        dum = len(text) / len(set(text))   # IMP, it has to be set
      except:
          print('exceptio while doing lexical_diversity, text is = ', text)
      return dum

  def percentage(count, total):
      ans = 0
      try:
        ans = 100 * count / total
      except:
       print('exceptio while doing lexical_diversity, tot is', total)

      return ans

  data['lexical_diversity'] = data['Domain'].apply(lexical_diversity)


  from nltk import pos_tag                                 
  def add_pos_tag(h):
      tag_token = pos_tag(h)
      return tag_token

  data['pos_tag'] = add_pos_tag(data['Domain'])

  data['is_adjective'] = data['pos_tag'].apply(lambda x: x[1] == 'JJ')
  data['is_coordinating_conjunction'] = data['pos_tag'].apply(lambda x: x[1] == 'CC')
  data['is_cardinal_digit'] = data['pos_tag'].apply(lambda x: x[1] == 'CD')
  data['is_determiner'] = data['pos_tag'].apply(lambda x: x[1] == 'DT')
  data['is_existential_there'] = data['pos_tag'].apply(lambda x: x[1] == 'EX')
  data['is_foreign_word'] = data['pos_tag'].apply(lambda x: x[1] == 'FW')

  data['is_preposition_conjunction'] = data['pos_tag'].apply(lambda x: x[1] == 'IN')
  data['is_adjective_comparative'] = data['pos_tag'].apply(lambda x: x[1] == 'JJR')
  data['is_adjective_superlative'] = data['pos_tag'].apply(lambda x: x[1] == 'JJS')
  data['is_list_market'] = data['pos_tag'].apply(lambda x: x[1] == 'LS')

  data['is_modal'] = data['pos_tag'].apply(lambda x: x[1] == 'MD')
  data['is_noun_singular'] = data['pos_tag'].apply(lambda x: x[1] == 'NN')
  data['is_noun_plural'] = data['pos_tag'].apply(lambda x: x[1] == 'NNS')
  data['is_proper_noun_singular'] = data['pos_tag'].apply(lambda x: x[1] == 'NNP')
  data['is_proper_noun_plural'] = data['pos_tag'].apply(lambda x: x[1] == 'NNPS')
  data['is_predeterminer'] = data['pos_tag'].apply(lambda x: x[1] == 'PDT')
  data['is_possessive_ending'] = data['pos_tag'].apply(lambda x: x[1] == 'POS')
  data['is_personal_pronoun'] = data['pos_tag'].apply(lambda x: x[1] == 'PRP')
  data['is_possessive_pronoun'] = data['pos_tag'].apply(lambda x: x[1] == 'PRP$')
  data['is_adverb'] = data['pos_tag'].apply(lambda x: x[1] == 'RB')
  data['is_adverb_comparative'] = data['pos_tag'].apply(lambda x: x[1] == 'RBR')
  data['is_adverb_superlative'] = data['pos_tag'].apply(lambda x: x[1] == 'RBS')
  data['is_particle'] = data['pos_tag'].apply(lambda x: x[1] == 'RP')
  data['is_infinite_marker'] = data['pos_tag'].apply(lambda x: x[1] == 'TO')
  data['is_interjection'] = data['pos_tag'].apply(lambda x: x[1] == 'UH')
  data['is_verb'] = data['pos_tag'].apply(lambda x: x[1] == 'VB')
  data['is_verb_gerund'] = data['pos_tag'].apply(lambda x: x[1] == 'VBG')
  data['is_verb_past_tense'] = data['pos_tag'].apply(lambda x: x[1] == 'VBD')
  data['is_verb_past_participle'] = data['pos_tag'].apply(lambda x: x[1] == 'VBN')
  data['is_verb_present'] = data['pos_tag'].apply(lambda x: x[1] == 'VBP')
  data['is_verb_present_3rd_person_singular'] = data['pos_tag'].apply(lambda x: x[1] == 'VBZ')
  data['is_wh_determiner'] = data['pos_tag'].apply(lambda x: x[1] == 'WDT')
  data['is_wh_pronoun'] = data['pos_tag'].apply(lambda x: x[1] == 'WP')
  data['is_wh_adverb'] = data['pos_tag'].apply(lambda x: x[1] == 'WRB')


  # Few more

  # Does it contains number
  def containsNumber(inputString):
      ans = any(char.isdigit() for char in inputString)
    # print('input = ', inputString, '; ans = ', ans)
      return ans

  # Number of vowels vowels
  def countVowels(inputString):
    return (len(re.findall(r'[aeiou]', inputString)))

  # Number of non-wovels
  def count_nonVowels(inputString):
    v =  (len(re.findall(r'[aeiou]', inputString)))
    tot = len(inputString)
    return (tot - v)

  # ratio of vowle to no vowel
  def count_Vowels_ratio(inputString):
    v =  (len(re.findall(r'[aeiou]', inputString)))
    tot = len(inputString)
    ratio = 0;
    try:
      ratio  = (v/tot)
    except: 
      print('divide by zero')  
    return (ratio)

  # Does two vowels occurrs twice in sequence?
  def  find_double_vowels(inputString):
    return (len(re.findall(r'[aeiou]{2}', inputString)))


  # Numebrs etc
  data['is_all_number'] = data['Domain'].apply(lambda x: x.isnumeric())
  data['is_all_alpha'] = data['Domain'].apply(lambda x: x.isalpha())
  data['is_alpha_numeric'] = data['Domain'].apply(lambda x: containsNumber(x))
  data['vowel_count'] = data['Domain'].apply(lambda x: countVowels(x))
  data['non_vowel_count'] = data['Domain'].apply(lambda x: count_nonVowels(x))
  data['vowel_ratio'] = data['Domain'].apply(lambda x: count_Vowels_ratio(x))
  data['double_vowel_sequence'] = data['Domain'].apply(lambda x: find_double_vowels(x))

"""## **Data Exploration**"""

#sns.set_theme(style="whitegrid", palette="pastel")

'''
dist.plot('categories', 'count', kind = 'bar')
plt.title('Distribution of Price Rankings of the domain names')
'''

#continuous_data = data[['DLength', 'DAlpha', 'DHaveDigits', 'DAllDigits', 'DHaveHyphen', 'DFirstDigit', 'DLastDigit', 'DVowelRatio', 'DConsonantRatio', 'DHyphenRatio', 'DDigitRatio', 'DShannonEntropy', 'DWordCount', 'DLanguageScore', 'category']]
continuous_data = data[['DLength', 'DAlpha', 'DHaveDigits', 'DAllDigits', 'DHaveHyphen', 'DFirstDigit', 'DLastDigit', 'DVowelRatio', 'DConsonantRatio', 'DHyphenRatio', 'DDigitRatio', 'DShannonEntropy', 'DWordCount', 'category']]
continuous_data.describe()

'''

sns.boxplot(data['DLength'])
plt.title('Boxplot of Domain Name Lengths')
'''

'''
binary = ['DHaveDigits', 'DAllDigits', 'DHaveHyphen', 'DFirstDigit', 'DLastDigit', 'DWordCount']
for i in binary:
  sns.countplot(data = data, x = 'category', hue = i)
  plt.title('Distribution of %s' %i)
  plt.show()
  print(data[i].value_counts()/data.shape[0]*100)
  '''

'''
sns.boxplot(data=data, x='category', y='DWordCount')
plt.title('Distribution of Word counts across the respective categories')
'''

'''
sns.boxplot(data=data, x='category', y='DLength')
plt.title('Distribution of Domain Name Lengths across the respective categories')
'''

'''
sns.pairplot(data = data, vars = ['DLength', 'DAlpha', 'DVowelRatio', 'DConsonantRatio', 'DHyphenRatio', 'DDigitRatio', 'DShannonEntropy', 'DLanguageScore'], hue = 'category')
'''

'''
continuous_data.corr()
'''

''
corr = continuous_data.corr()

mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

f, ax = plt.subplots(figsize=(12, 10))
sns.heatmap(corr, mask=mask, vmax=1, annot=True, fmt='.2f', square=False, linewidths=.5, cbar = True);
''

"""## **Data Cleaning**"""

# Identifying columns with high correlation (+ or - 0.9) with other columns as they will only introduce noise to the model
'''upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))
to_drop = [column for column in upper.columns if any(np.abs(upper[column]) > 0.9)]
to_drop
'''

# dropping the columns
'''
clean_data = data.drop(to_drop, axis = 1)

'''
clean_data = data
print(clean_data.head())

# Drop null values if any
print('We have %s rows before dropping null values' % clean_data.shape[0])
clean_data.dropna(subset = ['DName'], inplace = True)
print('We have %s rows after dropping null values' % clean_data.shape[0])

clean_data.isnull().sum()

"""There are no null values in the dataset

According to [Domain Name Rules](https://www.dynadot.com/community/blog/domain-name-rules.html#:~:text=Domain%20names%20are%20only%20allowed%20to%20be%2063,referred%20to%20as%20the%20%E2%80%98label%E2%80%99%29%20between%20the%20two.), we were also able to identify some criteria that all internet domain names must meet to be considered valid e.g. the correct domain extension, the length of characters must be 63 or less, domain names can contain numbers but the only permitted special character is the hyphen '-'. We will check validity of the domain names provided against these rules

**Step 2**: Check validity of domain (domain extension)
"""

#clean_data['DExt'].value_counts()

"""All domain extensions are valid. All our domains have the same extension '.com' so we will drop this column to reduce the noise in the data

**Step 3**: Check the length of the domain name to ensure it is less than 63 characters
"""

#clean_data[clean_data['DLength'] > 63].shape[0]
# There are no domain names with lengths greater than 63

# also dropping the outliers identified in the DLength column i.e. domain names with more than 25 characters
#clean_data = clean_data[clean_data['DLength'] <= 25]

'''
 # Number of special characters
 sc = clean_data['DName'].apply(lambda x: len(re.findall('[\W]', x))).sum()
 # Number of hyphens
 hp = clean_data['DName'].apply(lambda x: len(re.findall('[-]', x))).sum()
 # Number of special characters excluding hyphens
 sc - hp
 '''

"""## **Feature Selection**"""

# Here the session crashes due to issue of tolist etc
# decomposing the word vectors from 300 to 5

#print(clean_data['DVector'])


pd.set_option('display.max_columns', None)
clean_data.head(10)


if(IS_ALL_DATA ==False):
  pca = PCA(n_components = 5)
  outcome = pca.fit_transform(pd.DataFrame(np.vstack(clean_data['DVector'].values)))
  pca_df = pd.DataFrame(data = outcome, columns = ['DPCA1', 'DPCA2', 'DPCA3', 'DPCA4', 'DPCA5'
                                                  ])

  pca_df.shape

clean_data.shape

if(IS_ALL_DATA ==False):
  # concatinating the data with the pca columns
  all_data = clean_data.reset_index().join(pca_df)
  # dropping the original vector column
  all_data.drop(['DVector'], inplace = True, axis = 1)
  all_data.isnull().sum()

else:  
  all_data = clean_data

pd.set_option('display.max_columns', None)

all_data.head()

# define a function to measure the variance of our data features  
'''
from sklearn.feature_selection import VarianceThreshold
def variance_threshold_selector(data, threshold):
  selector = VarianceThreshold(threshold)
  selector.fit(data)
  return data[data.columns[selector.get_support(indices=True)]]
  
filtered_X = variance_threshold_selector(X, threshold=0.02)
filtered_X.var()
'''

filtered_X = all_data

# splitting the data into dependent and independent variables
y = filtered_X['category']

if(IS_ALL_DATA ==False):
    #X = filtered_X[col_Features]
    X = filtered_X[col_Feature_No_PCA]

else:
  X = filtered_X[col_Feature_No_PCA]



X = X.replace({False: 0, True: 1})  ######################################### IMP TO Replace T/F into 1/0

pd.set_option('display.max_columns', None)
X.head()

"""## **Model Building**"""

pd.set_option('display.max_columns', None)
X.head(10)

# splitting the dataset
train_x, val_x, train_y, val_y = train_test_split(X, y, test_size = 0.02, random_state = 0, stratify = y)
train_x.shape, val_x.shape


'''
print(train_x)
print(train_x["DPCA1"].unique())
'''

# scaling the data using a minmaxscaler as we have some negative values in our PCA features

scaler = MMS(feature_range=(0, 1))
scaled_train_x = scaler.fit_transform(train_x)
scaled_val_x = scaler.transform(val_x)


#scaled_train_x = train_x
#scaled_val_x = val_x

"""### **Logistic Regression**"""



"""### **TREE**"""

dtc = DTC(criterion = 'gini', splitter = 'random', class_weight='balanced')  # We might need to change this 
dtc.fit(scaled_train_x, train_y)
pred_y = dtc.predict(scaled_val_x)

print('The precision using Decision Trees before hyperparameter tuning is', precision_score(val_y, pred_y, average = 'weighted'))
print('The recall using Decision Trees before hyperparameter tuning is', recall_score(val_y, pred_y, average = 'weighted'))
print('The accuracy score using Decision Trees before hyperparameter tuning is', accuracy_score(val_y, pred_y))
print('The F1 score using Decision Trees before hyperparameter tuning is', f1_score(val_y, pred_y, average = 'weighted'), '\n')
confusion_matrix(val_y, pred_y)
fig, ax = plt.subplots(figsize=(8,8))
sns.heatmap(confusion_matrix(val_y, pred_y), annot = True, cmap = "Blues", fmt = 'd', xticklabels = [1, 2, 3, 4, 5], yticklabels = [1, 2, 3, 4, 5])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title("CONFUSION MATRIX - Decision Trees\n", size = 16);

# Save the Model
import pickle


filename=''

# save the model to disk
if(IS_ALL_DATA ==False):
  filename = DT_JAIN_600
else:
  filename = DT_ALL_NOPACE

pickle.dump(dtc, open(filename, 'wb'))

"""### **LR**"""

lr = LR(penalty = 'l2', random_state = 0, solver = 'saga', class_weight='balanced')
lr.fit(scaled_train_x, train_y)
pred_y = lr.predict(scaled_val_x)
print('The precision using Logisitic Regression before hyperparameter tuning is', precision_score(val_y, pred_y, average = 'weighted'))
print('The recall using Logisitic Regression before hyperparameter tuning is', recall_score(val_y, pred_y, average = 'weighted'))
print('The accuracy score using Logisitic Regression before hyperparameter tuning is', accuracy_score(val_y, pred_y))
print('The F1 score using Logisitic Regression before hyperparameter tuning is', f1_score(val_y, pred_y, average = 'weighted'), '\n')
fig, ax = plt.subplots(figsize=(8,8))
sns.heatmap(confusion_matrix(val_y, pred_y), annot = True, cmap = "Blues", fmt = 'd', xticklabels = [1, 2, 3, 4, 5], yticklabels = [1, 2, 3, 4, 5])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title("CONFUSION MATRIX - Logistic Regression\n", size = 16);


# Save the Model
import pickle

filename=''

# save the model to disk
if(IS_ALL_DATA ==False):
  filename = LR_JAIN_600
else:
  filename = LR_ALL_NOPACE
  
pickle.dump(lr, open(filename, 'wb'))

from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.datasets import make_classification



clf = BaggingClassifier(base_estimator= dtc, n_estimators=10, random_state=0)


clf.fit(scaled_train_x, train_y)
y_preds = clf.predict(scaled_val_x)

print('The precision using bag  before hyperparameter tuning is', precision_score(val_y, pred_y, average = 'weighted'))
print('The recall using bag  before hyperparameter tuning is', recall_score(val_y, pred_y, average = 'weighted'))
print('The accuracy score using bag  before hyperparameter tuning is', accuracy_score(val_y, pred_y))
print('The F1 score using bag  before hyperparameter tuning is', f1_score(val_y, pred_y, average = 'weighted'), '\n')
confusion_matrix(val_y, pred_y)
fig, ax = plt.subplots(figsize=(8,8))
sns.heatmap(confusion_matrix(val_y, pred_y), annot = True, cmap = "Blues", fmt = 'd', xticklabels = [1, 2, 3, 4, 5], yticklabels = [1, 2, 3, 4, 5])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title("CONFUSION MATRIX - clf\n", size = 16);


# Save the Model
import pickle

filename=''

# save the model to disk
if(IS_ALL_DATA ==False):
  filename = BAG_JAIN_600
else:
  filename = BAG_ALL_NOPACE

pickle.dump(clf, open(filename, 'wb'))

'''
#svc = SVC(kernel = 'rbf')
svc = SVC(class_weight='balanced')

svc.fit(scaled_train_x, train_y)
pred_y = svc.predict(scaled_val_x)
print('The precision using Support Vector Machine before hyperparameter tuning is', precision_score(val_y, pred_y, average = 'weighted'))
print('The recall using Support Vector Machine before hyperparameter tuning is', recall_score(val_y, pred_y, average = 'weighted'))
print('The accuracy score using Support Vector Machine before hyperparameter tuning is', accuracy_score(val_y, pred_y))
print('The F1 score using Support Vector Machine before hyperparameter tuning is', f1_score(val_y, pred_y, average = 'weighted'), '\n')
fig, ax = plt.subplots(figsize=(8,8))
sns.heatmap(confusion_matrix(val_y, pred_y), annot = True, cmap = "Blues", fmt = 'd', xticklabels = [1, 2, 3, 4, 5], yticklabels = [1, 2, 3, 4, 5])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title("CONFUSION MATRIX - Support Vector Machines\n", size = 16);

# Save the Model
import pickle

# save the model to disk
filename = saved_models_path_new + 'SVC_finalized_model_Jain_600.sav'
pickle.dump(svc, open(filename, 'wb'))

'''

##################################

"""## **Hyperparameter Tuning**"""

'''
lr_param_grid = {'penalty': ['l1', 'l2', 'elasticnet'], 'solver': ['saga'], 'random_state': [0], 'C':[1, 10, 20, 50]}
svc_param_grid = {'kernel':('linear', 'rbf'), 'C':[1, 10, 20, 50]}
dtc_param_grid = {'criterion': ['gini', 'entropy'], 'splitter':['best', 'random'], 'max_depth': [i for i in range (1, 11, 2)]}

'''

'''
lr_gs = GridSearchCV(lr, lr_param_grid, cv = 4)
lr_gs.fit(scaled_train_x, train_y)
lr_gs.best_params_
'''

'''
svc_gs = GridSearchCV(svc, svc_param_grid, cv = 4)
svc_gs.fit(scaled_train_x, train_y)
svc_gs.best_params_
'''

'''
dtc_gs = GridSearchCV(dtc, dtc_param_grid, cv = 4)
dtc_gs.fit(scaled_train_x, train_y)
dtc_gs.best_params_
'''

"""## **Final Results**"""

'''
lr = LR(C = 1, penalty = 'l1', random_state = 0, solver = 'saga')
lr.fit(scaled_train_x, train_y)
pred_y = lr.predict(scaled_val_x)
print('The precision using Logisitic Regression after hyperparameter tuning is', precision_score(val_y, pred_y, average = 'weighted'))
print('The recall using Logisitic Regression after hyperparameter tuning is', recall_score(val_y, pred_y, average = 'weighted'))
print('The accuracy score using Logisitic Regression after hyperparameter tuning is', accuracy_score(val_y, pred_y))
print('The F1 score using Logisitic Regression after hyperparameter tuning is', f1_score(val_y, pred_y, average = 'weighted'), '\n')
fig, ax = plt.subplots(figsize=(8,8))
sns.heatmap(confusion_matrix(val_y, pred_y), annot = True, cmap = "Blues", fmt = 'd', xticklabels = [1, 2, 3, 4, 5], yticklabels = [1, 2, 3, 4, 5])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title("CONFUSION MATRIX - Logistic Regression\n", size = 16);
'''

'''
print('The Logistic Regression intercept value is', lr.intercept_[0], '\n')
plt.barh(train_x.columns[1:], lr.coef_[0][1:], align='center')
plt.title('Feature Importance for the Logistic Regression Classifier')
plt.show()
'''

'''
svc = SVC(C = 50, kernel = 'linear')
svc.fit(scaled_train_x, train_y)
pred_y = svc.predict(scaled_val_x)
print('The precision using Support Vector Machine after hyperparameter tuning is', precision_score(val_y, pred_y, average = 'weighted'))
print('The recall using Support Vector Machine after hyperparameter tuning is', recall_score(val_y, pred_y, average = 'weighted'))
print('The accuracy score using Support Vector Machine after hyperparameter tuning is', accuracy_score(val_y, pred_y))
print('The F1 score using Support Vector Machine after hyperparameter tuning is', f1_score(val_y, pred_y, average = 'weighted'), '\n')
fig, ax = plt.subplots(figsize=(8,8))
sns.heatmap(confusion_matrix(val_y, pred_y), annot = True, cmap = "Blues", fmt = 'd', xticklabels = [1, 2, 3, 4, 5], yticklabels = [1, 2, 3, 4, 5])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title("CONFUSION MATRIX - Support Vector Machines\n", size = 16);
'''

'''
plt.barh(train_x.columns[1:], svc.coef_[0][1:], align='center')
plt.title('Feature Importance for the SVC Classifier')
plt.show()
'''

'''
dtc = DTC(criterion = 'gini', max_depth = 5, splitter = 'best')
dtc.fit(scaled_train_x, train_y)
pred_y = dtc.predict(scaled_val_x)
print('The precision using Decision Trees after hyperparameter tuning is', precision_score(val_y, pred_y, average = 'weighted'))
print('The recall using Decision Trees after hyperparameter tuning is', recall_score(val_y, pred_y, average = 'weighted'))
print('The accuracy score using Decision Trees after hyperparameter tuning is', accuracy_score(val_y, pred_y))
print('The F1 score using Decision Trees after hyperparameter tuning is', f1_score(val_y, pred_y, average = 'weighted'), '\n')
confusion_matrix(val_y, pred_y)
fig, ax = plt.subplots(figsize=(8,8))
sns.heatmap(confusion_matrix(val_y, pred_y), annot = True, cmap = "Blues", fmt = 'd', xticklabels = [1, 2, 3, 4, 5], yticklabels = [1, 2, 3, 4, 5])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title("CONFUSION MATRIX - Decision Trees\n", size = 16);
'''

from sklearn.tree import plot_tree
plt.figure(figsize=(10, 5))
plot_tree(dtc)
plt.show()
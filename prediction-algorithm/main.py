# -*- coding: utf-8 -*-
"""Test_File_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eri81_QSzuVH1azHlNOK5JHOdMuJ7B8v
"""

# Loading some libraries
# This step might takes few mins

# from google.colab import drive
# drive.mount('/content/drive')

# Importing the relevant libraries

import os 
import sys
import json
currentDirectoryPATH = os.path.dirname(os.path.realpath(__file__))

import pandas as pd
import re
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings(action='ignore')
import collections
from scipy.stats import entropy
import nltk
from nltk.corpus import words, stopwords
nltk.download('words')
nltk.download('stopwords')
# !pip install langid
from langid.langid import LanguageIdentifier, model
import gensim.downloader as api
# !pip install wordninja
import wordninja
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression as LR
from sklearn.svm import SVC
# !pip install blosc
# !pip install pickle 

from weakref import WeakValueDictionary
from gensim import models
# !pip install gensim # to upgrade version
from gensim.models import Word2Vec
import gensim

import pickle
import blosc




import nltk
nltk.download('gutenberg')
nltk.download('genesis')
nltk.download('inaugural')
nltk.download('nps_chat')
nltk.download('webtext')
nltk.download('treebank')
nltk.download('brown')
nltk.download('averaged_perceptron_tagger')
nltk.download("stopwords")
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('words')

import nltk.corpus
from nltk.book import *
from nltk.corpus import brown

english_vocab = set(w.lower() for w in nltk.corpus.words.words())
# !pip install tld
from tld import get_tld     # For Extracting TLD

# !pip install embeddings  # from pypi
# !pip install git+https://github.com/vzhong/embeddings.git  # from github

from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler as MMS
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

import csv

################################################
# Test Results
ACCURACY = 0.0
ERROR_READING = False
ERROR_PATH    = False


###############################################
# Final Features



  # PCA Featurs (Jains) 

Col_Features =   ['DExtension', 'DLength', 'DAlpha', 'DHaveDigits', 'DAllDigits', 'DHaveHyphen', 
                  'DFirstDigit','DLastDigit', 'DVowelRatio', 'DConsonantRatio', 'DHyphenRatio', 
                'DDigitRatio', 'DShannonEntropy' ,'DWordCount', 'DLanguageScore' ]



        
'''

col_Feature_No_PCA =  ['DExtension',  'is_wh_adverb', 'is_wh_pronoun', 
                       'is_wh_determiner', 'is_verb_present_3rd_person_singular', 
                     'is_verb_present', 'is_verb_past_participle', 'is_verb_past_tense',
                     'is_verb_gerund', 'is_verb', 'is_interjection', 'is_infinite_marker', 
                     'is_particle', 'is_adverb_superlative','is_adverb_comparative', 
                     'is_adverb', 'is_possessive_pronoun', 
                     'is_personal_pronoun', 'is_possessive_ending',
                    'is_predeterminer', 'is_proper_noun_plural', 
                    'is_proper_noun_singular', 'is_noun_plural', 
                    'is_noun_singular','is_modal', 'is_modal',
                    'is_list_market', 'is_adjective_superlative', 
                    'is_adjective_comparative', 'is_preposition_conjunction',
                    'is_foreign_word', 'is_existential_there', 
                    'is_determiner', 'is_cardinal_digit', 
                    'is_coordinating_conjunction', 
                    'is_adjective', 'lexical_diversity', 
                    'dictionary_words_count', 'dictionary_words_present', 
                    'character_type',
                    'words_len_min',  # 
                    
                  'DLength', 'DAlpha', 'DHaveDigits', 'DAllDigits', 'DHaveHyphen', 
                  'DFirstDigit','DLastDigit', 'DVowelRatio', 'DConsonantRatio', 'DHyphenRatio', 
                  'DDigitRatio', 'DShannonEntropy' ,'DWordCount', 'DLanguageScore'] #55


'''



Col_Output  = 'category'

IS_DEBUG = True # TO see some log etc

#######################################################################################################################################
# 1- Test File Path
# 2-  Where to store results
# 3 - Model File Path
############################
# Test File paths
############################
# Make sure to change these paths as per server path


##################################################
# INPUT FILE PATH
##################################################
# Give the INPUT FILE PATH

# In google drive, the path start from this:

google_world_path = currentDirectoryPATH + '/trained_models/GoogleNews-vectors-negative300.bin'




##################################################
# OUTPUT FOLDER PATH
##################################################

# Give the "FOLDER PATH", WHERE you want results (It needed to be folder only --- DONT GIVE FILE NAME)

OUTPUT_FOLDER = currentDirectoryPATH + '/results/'
  

##################################################
# SAVED MODEL PATH
##################################################
# make sure to change this path as needed (models are saved in the code shared)

#saved_models_path = '/content/drive/MyDrive/Domain_Name_Prediction_Final/Code/SavedModels/NN_HASHModel/Model_new'

# If you are in computer, it should something like this one:
saved_models_path = currentDirectoryPATH + '/trained_models/'

saved_models_DT_600_pca = saved_models_path + 'DT_finalized_model_Jain_600.sav'
saved_models_LR_600_pca = saved_models_path + 'LR_finalized_model_Jain_600.sav'
saved_models_BAG_600_pca = saved_models_path + 'BAG_finalized_model_Jain_600.sav'


#from gensim.models import Word2Vec

wv = gensim.models.KeyedVectors.load_word2vec_format(google_world_path, binary=True)



####################################################################################
# Dont worry about these paths, they will use above path to adjust automatically
###################################################################################

# result file will be generated automatically 
file_to_save_results = OUTPUT_FOLDER + 'Results.xlsx'
file_to_save_features = OUTPUT_FOLDER + 'testingdata_prep.xlsx'


# Result File
result_path = OUTPUT_FOLDER
result_path_accuracy = OUTPUT_FOLDER + 'accuracy.png'
result_path_errors = OUTPUT_FOLDER + 'errors.png'
result_path_confusion = OUTPUT_FOLDER + 'confusion.png'

# Making sure, we have data in right format
#INPUT_PATH = '/content/drive/MyDrive/Domain_Name_Prediction_Final_Processing/Data/TestData/sample_test_data_diverse4.csv'

#INPUT_PATH = '/content/drive/MyDrive/Domain_Name_Prediction_Final_Processing/Data/TestData/TestDataCSV200.csv'

INPUT_PATH = (currentDirectoryPATH + "/" + sys.argv[1])


# test file
test_file = INPUT_PATH

data = pd.read_csv(INPUT_PATH, header = None)

##################################################################################
# If file is empty, exit
if (data.shape[0] ==0):
  print('Empty file, pl upload file')
  exit()
print('org data is ')
print(data)

print('colm is')
print(data.columns[1])


data.rename(columns={data.columns[0]:'Domain'}, inplace=True)
data.rename(columns={data.columns[1]:'category'}, inplace=True)

# deleting first row, if this is header, as we are crarting columns names in the code
if (data.columns[1]  == str.lower('category')):
  data = data.drop(labels=0, axis=0) 
  print('error')




print('Data is ==')
print (data)
##################################################################################
# 1- Pls check, if the user is adding CSV file only
# Write some code here

def get_Domain_Extension (one_value):
  ext = 'unknown'
  try:
    ext = get_tld(one_value, fix_protocol=True)
  except:
    print('cant find domain extension..')

  return (ext)

def get_Domain (one_value):
  dom = one_value
  try:
    ext = get_tld(one_value, fix_protocol=True, as_object=True)
    dom  =str(ext.domain)
  except:
    print('cant find domain')
  return dom

data['DExtension'] = data['Domain'].apply(lambda x:  get_Domain_Extension(x))
data['host'] = data['Domain'].apply(lambda x:  get_Domain(x))


# Appyl lable encoding for extension
le = preprocessing.LabelEncoder()
data['DExtension'] = le.fit_transform(data['DExtension'])

data['Domain'] = data['host']     # This will get rid f extension

'''
print(data['DExtension'])

print(data['host'])
print(data['Domain'])
'''

dist = data.category.value_counts().to_frame().reset_index()
dist['% distribution'] = data.category.value_counts().values/data.shape[0] * 100
dist.columns = ['categories', 'count', '% distribution']
dist


#############

# Extracting the domain name
data['DName'] = data['Domain'].apply(lambda x: x.split('.')[0].lower())

# Extracting the domain extension
#data['DExt'] = data['Domain'].apply(lambda x: x.split('.')[1])

# Counting the number of characters in the domain name
data['DLength'] = data['DName'].apply(lambda x: len(x))

# Extracting all alphabets in the domain name and counting the number of alphabets in the string
data['DAlphaChar'] = data['DName'].apply(lambda x: ''.join(re.findall('[a-zA-Z-]', x)))
data['DAlpha'] = data['DAlphaChar'].apply(lambda x: len(x))

# Are there digits in the domain name
data['DHaveDigits'] = data['DName'].apply(lambda x: 0 if len(re.findall('\d', x)) == 0 else 1)
data['DAllDigits'] = data.apply(lambda x: 1 if len(re.findall('\d', x['DName'])) == x['DLength'] else 0, axis = 1)

# Are there hyphens in the domain name
data['DHaveHyphen'] = data['DName'].apply(lambda x: 0 if len(re.findall('[-]', x)) == 0 else 1)

# Checking if the first character of the domain name is an alphabet or digit: 1 if digit, 0 if alphabet
data['DFirstDigit'] = data['DName'].apply(lambda x: 1 if x[0].isdigit() else 0)

# Checking if the last character of the domain name is an alphabet or digit: 1 if digit, 0 if alphabet
data['DLastDigit'] = data['DName'].apply(lambda x: 1 if x[-1].isdigit() else 0)

# Define UDF to compute the ratio of vowels and consonants relative to the length of alphabets in the domain name
def extract_ratio(x, arg):
  if x['DAlpha'] == 0:
    return 0
  elif arg == 'vowel':
    return len(re.findall('[aeiou]', x['DName']))/x['DLength']
  elif arg == 'cons':
    return len(re.findall('[^aeiou0-9-]', x['DName']))/x['DLength']

# Compute the ratio of vowels to total alphabets (reflects pronouncability)
data['DVowelRatio'] = data.apply(lambda x: extract_ratio(x, 'vowel'), axis = 1)

# Compute the ratio of consonants to total alphabets
data['DConsonantRatio'] = data.apply(lambda x: extract_ratio(x, 'cons'), axis = 1)

# Compute the ratio of underscore
data['DHyphenRatio'] = data.apply(lambda x: len(re.findall('[-]', x['DName']))/x['DLength'], axis = 1)

# Compute the ratio of underscore
data['DDigitRatio'] = data.apply(lambda x: len(re.findall('\d', x['DName']))/x['DLength'], axis = 1)


#################
# define a UDF which computes the shannon entropy of any given string
def shannon_entropy(d_name):
    # count occurence of each character in the string
    xters = collections.Counter([txt for txt in d_name])
    # compute the ratio of each character relative to the length of the string
    dist = [x/sum(xters.values()) for x in xters.values()]
    # compute the entropy
    shan_entropy = entropy(dist, base=2)
 
    return shan_entropy

data['DShannonEntropy'] = data['DName'].apply(lambda x: shannon_entropy(x))
########################
# extract all meaningful words from the domain name if any using the wordninja library
data['DWords'] = data['DAlphaChar'].apply(lambda x: x.split('-') if '-' in x and len(x) > 5 else wordninja.split(x))

# define a function to remove stopwords and single letters from a list of words
def clean_LOW(lst):
  # list of english stop words
  stops = list(set(stopwords.words('english')))
  # list of english alphabets
  alphabets = [chr(x) for x in range(ord('a'), ord('z') + 1)]
  # set of stopwords in list of words
  stops_in_lst = set(lst).intersection(set(stops))
  # set of alphabets in list of words
  remove = set(lst).intersection(set(alphabets).union(set(stops)))
  for i in remove:
    if i in lst:
      lst.remove(i)
  for i in lst:
    if len(i) <= 2:
      lst.remove(i)
  return lst

data['DWords'] = data['DWords'].apply(lambda x: clean_LOW(x))
# count number of meaningful words in the domain name
data['DWordCount'] = data['DWords'].apply(lambda x: len(x))
data['DWordCount'].max()

cnt = data[data['DWordCount'] > 5].shape[0]
print('There are %d domain names in the dataset with 5 or more meaningful words' %cnt)

###############################
# Define a function that identified the domain names and classifies them to different languages. This may be considered the language score of the domain name i.e. a
# high language score means that the domain name is recognizable as english while a low score or score of zero means the domain name is gibberish 
identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)

# define a function that identifies each word in the string and computes a language score where the language is english
def lang_score(x):
  score = 0
  if len(x) == 0:
    pass
  else:
    for i in x:
      lang_and_score = identifier.classify(i)
      if lang_and_score[0] == 'en':
        score =+ lang_and_score[1]
    score = score/len(x)    
  return score

data['DLanguageScore'] = data['DWords'].apply(lambda x: lang_score(x))

#######################

# Loading in the pre-trained google word 2 vec 

# define function to obtain the average vector for each group of words
def vectorize(x):
  vec = np.zeros(300)
  if x['DWords'] == []:
    pass
  else:
    for i in x['DWords']:
      try:
        vec =+ wv[i]
      except:
        pass
    vec = vec/x['DWordCount']
  return vec

data['DVector'] = data.apply(lambda x: vectorize(x), axis = 1)


pd.set_option('display.max_columns', None)
print(data["DVector"])

###############################################################################
# From Madiha
###############################################################################

'''
data['words'] = data['Domain'].apply(wordninja.split)


def calculate_words_len(val):
  if val:
    return [len(word) for word in val]
  return []

# Will it helps, if we know, the word size is big which is found?
def calculate_words_len_min(val):
  length = 0
  if val:
   for word in val:
     lenght = length+ len(word)
  return lenght

data['words_len'] = data['words'].apply(calculate_words_len)
data['words_len_min'] = data['words'].apply(calculate_words_len_min)

def check_type(val: str):
  if val.isdigit():
    return 1
  if val.isalpha():
    return 2
  return 3

data['character_type'] = data['Domain'].apply(check_type)

from nltk.corpus import wordnet as wn  # Takes some time

def get_dict_words(val):
  new_list = []
  for word in val:
    if word and not word.strip(word[0]):
      continue

    temp_l = wn.synsets(word)
    if temp_l and (word in english_vocab or temp_l[0].pos()) and not word.isdigit():
      new_list.append(word)
  return new_list

data['dict_words'] = data['words'].apply(get_dict_words)

data['dictionary_words_count'] = data['dict_words'].apply(lambda l: len(l))  # (3 sec)
data['dictionary_words_present']  = data['dictionary_words_count']
data['dictionary_words_present'].values[data['dictionary_words_present'].values > 1] = 1 

def lexical_diversity(text):
    dum = 0
    try:
      dum = len(text) / len(set(text))   # IMP, it has to be set
    except:
        print('exceptio while doing lexical_diversity, text is = ', text)
    return dum

def percentage(count, total):
    ans = 0
    try:
      ans = 100 * count / total
    except:
     print('exceptio while doing lexical_diversity, tot is', total)

    return ans

data['lexical_diversity'] = data['Domain'].apply(lexical_diversity)


from nltk import pos_tag                                 
def add_pos_tag(h):
    tag_token = pos_tag(h)
    return tag_token

data['pos_tag'] = add_pos_tag(data['Domain'])

data['is_adjective'] = data['pos_tag'].apply(lambda x: x[1] == 'JJ')
data['is_coordinating_conjunction'] = data['pos_tag'].apply(lambda x: x[1] == 'CC')
data['is_cardinal_digit'] = data['pos_tag'].apply(lambda x: x[1] == 'CD')
data['is_determiner'] = data['pos_tag'].apply(lambda x: x[1] == 'DT')
data['is_existential_there'] = data['pos_tag'].apply(lambda x: x[1] == 'EX')
data['is_foreign_word'] = data['pos_tag'].apply(lambda x: x[1] == 'FW')

data['is_preposition_conjunction'] = data['pos_tag'].apply(lambda x: x[1] == 'IN')
data['is_adjective_comparative'] = data['pos_tag'].apply(lambda x: x[1] == 'JJR')
data['is_adjective_superlative'] = data['pos_tag'].apply(lambda x: x[1] == 'JJS')
data['is_list_market'] = data['pos_tag'].apply(lambda x: x[1] == 'LS')

data['is_modal'] = data['pos_tag'].apply(lambda x: x[1] == 'MD')
data['is_noun_singular'] = data['pos_tag'].apply(lambda x: x[1] == 'NN')
data['is_noun_plural'] = data['pos_tag'].apply(lambda x: x[1] == 'NNS')
data['is_proper_noun_singular'] = data['pos_tag'].apply(lambda x: x[1] == 'NNP')
data['is_proper_noun_plural'] = data['pos_tag'].apply(lambda x: x[1] == 'NNPS')
data['is_predeterminer'] = data['pos_tag'].apply(lambda x: x[1] == 'PDT')
data['is_possessive_ending'] = data['pos_tag'].apply(lambda x: x[1] == 'POS')
data['is_personal_pronoun'] = data['pos_tag'].apply(lambda x: x[1] == 'PRP')
data['is_possessive_pronoun'] = data['pos_tag'].apply(lambda x: x[1] == 'PRP$')
data['is_adverb'] = data['pos_tag'].apply(lambda x: x[1] == 'RB')
data['is_adverb_comparative'] = data['pos_tag'].apply(lambda x: x[1] == 'RBR')
data['is_adverb_superlative'] = data['pos_tag'].apply(lambda x: x[1] == 'RBS')
data['is_particle'] = data['pos_tag'].apply(lambda x: x[1] == 'RP')
data['is_infinite_marker'] = data['pos_tag'].apply(lambda x: x[1] == 'TO')
data['is_interjection'] = data['pos_tag'].apply(lambda x: x[1] == 'UH')
data['is_verb'] = data['pos_tag'].apply(lambda x: x[1] == 'VB')
data['is_verb_gerund'] = data['pos_tag'].apply(lambda x: x[1] == 'VBG')
data['is_verb_past_tense'] = data['pos_tag'].apply(lambda x: x[1] == 'VBD')
data['is_verb_past_participle'] = data['pos_tag'].apply(lambda x: x[1] == 'VBN')
data['is_verb_present'] = data['pos_tag'].apply(lambda x: x[1] == 'VBP')
data['is_verb_present_3rd_person_singular'] = data['pos_tag'].apply(lambda x: x[1] == 'VBZ')
data['is_wh_determiner'] = data['pos_tag'].apply(lambda x: x[1] == 'WDT')
data['is_wh_pronoun'] = data['pos_tag'].apply(lambda x: x[1] == 'WP')
data['is_wh_adverb'] = data['pos_tag'].apply(lambda x: x[1] == 'WRB')


# Few more

# Does it contains number
def containsNumber(inputString):
    ans = any(char.isdigit() for char in inputString)
   # print('input = ', inputString, '; ans = ', ans)
    return ans

# Number of vowels vowels
def countVowels(inputString):
  return (len(re.findall(r'[aeiou]', inputString)))

# Number of non-wovels
def count_nonVowels(inputString):
  v =  (len(re.findall(r'[aeiou]', inputString)))
  tot = len(inputString)
  return (tot - v)

# ratio of vowle to no vowel
def count_Vowels_ratio(inputString):
  v =  (len(re.findall(r'[aeiou]', inputString)))
  tot = len(inputString)
  ratio = 0;
  try:
   ratio  = (v/tot)
  except: 
    print('divide by zero')  
  return (ratio)

# Does two vowels occurrs twice in sequence?
def  find_double_vowels(inputString):
  return (len(re.findall(r'[aeiou]{2}', inputString)))


# Numebrs etc
data['is_all_number'] = data['Domain'].apply(lambda x: x.isnumeric())
data['is_all_alpha'] = data['Domain'].apply(lambda x: x.isalpha())
data['is_alpha_numeric'] = data['Domain'].apply(lambda x: containsNumber(x))
data['vowel_count'] = data['Domain'].apply(lambda x: countVowels(x))
data['non_vowel_count'] = data['Domain'].apply(lambda x: count_nonVowels(x))
data['vowel_ratio'] = data['Domain'].apply(lambda x: count_Vowels_ratio(x))
data['double_vowel_sequence'] = data['Domain'].apply(lambda x: find_double_vowels(x))

'''

####################

clean_data = data
#print(clean_data.head())

################
# Drop null values if any
print('We have %s rows before dropping null values' % clean_data.shape[0])
clean_data.dropna(subset = ['DName'], inplace = True)
print('We have %s rows after dropping null values' % clean_data.shape[0])
clean_data.isnull().sum()
################

##################

# Here the session crashes due to issue of tolist etc
# decomposing the word vectors from 300 to 5

#print(clean_data['DVector'])

pd.set_option('display.max_columns', None)
clean_data.head(10)

COMPONENTS = len(clean_data)

if(COMPONENTS >5):
  COMPONENTS = 5

# define pca features, in case someone enters less than 5 ddomains as test set
PCA_COMP_NAMES = ["DPCA"+str(x+1) for x in range(COMPONENTS)]

# DO PCA with 5 components
pca = PCA(n_components = COMPONENTS)
outcome = pca.fit_transform(pd.DataFrame(np.vstack(clean_data['DVector'].values)))
pca_df = pd.DataFrame(data = outcome, columns = PCA_COMP_NAMES)
#######################################################################
# Adding more columns
PCA_COMP_NAMES_REMAINING =[]

if(COMPONENTS <5):
  REMAINING_COMPONENTS = 5- COMPONENTS 
 
  # define pca features, in case someone enters less than 5 ddomains as test set
  PCA_COMP_NAMES_REMAINING = ["DPCA"+str(COMPONENTS+x + 1) for x in range(REMAINING_COMPONENTS)] 

  for comp in PCA_COMP_NAMES_REMAINING:
    pca_df[comp] = 0

Col_Features = Col_Features + PCA_COMP_NAMES + PCA_COMP_NAMES_REMAINING

print(pca_df.shape)
print(clean_data.shape)
    
###########################
# concatinating the data with the pca columns
all_data = clean_data.reset_index().join(pca_df)

# dropping the original vector column
all_data.drop(['DVector'], inplace = True, axis = 1)
all_data.isnull().sum()

############################
print('====================================================')
print('DCAs are ')
print(pca_df.head())

# splitting the data into dependent and independent variables

# also dropping text features
X = all_data

print('====================================================')
print('X')
print('====================================================')

pd.set_option('display.max_columns', None)
print(X.head(10))
##########################
# For test set we cant drop any columns  

from sklearn.feature_selection import VarianceThreshold

#############################
# scaling the data using a minmaxscaler as we have some negative values in our PCA features

filtered_X = X;
pd.set_option('display.max_columns', None)
print(filtered_X.head())

    
final_X  = filtered_X [Col_Features]
y        = filtered_X [Col_Output]

pd.set_option('display.max_columns', None)
final_X.head()

'''
print('=========================================')
print('FINAL X')
print('=========================================')
print(final_X)
'''


print('=========================================')
print('FINAL y')
print('=========================================')
print(y)

scaler = MMS(feature_range=(0, 1))
final_X =  scaler.fit_transform(final_X)



##############################

###########################################
# Load saved models
###########################################


model_loaded_lr_jain = pickle.load(open(saved_models_LR_600_pca, 'rb')) # Linear Regression
model_loaded_dt_jain = pickle.load(open(saved_models_DT_600_pca, 'rb')) # Decision Tree
model_loaded_bag_jain = pickle.load(open(saved_models_BAG_600_pca, 'rb')) # Bagging Algorithm with DT as base algorithm

# Make model_Predictions
import random

model_predictions_lr_jain = model_loaded_lr_jain.predict(final_X)
model_predictions_dt_jain = model_loaded_dt_jain.predict(final_X)
model_predictions_bags_jain = model_loaded_bag_jain.predict(final_X)


# Take maximum of predictions
model_predictions = np.maximum (model_predictions_lr_jain, model_predictions_dt_jain,model_predictions_bags_jain)

print(model_predictions)

dummy_output = filtered_X
dummy_output ['model_Predictions'] = model_predictions
final_outputs = dummy_output [['Domain', 'category', 'model_Predictions']]
print(final_outputs.head())


################################
#########################################
# Get Wrong model_predictions
#########################################
All = []
All_Wrong  = []

# For wrong only
for row_index, (input, prediction, label) in enumerate(zip (final_X, model_predictions, y)):
  msg_all = 'Row', row_index, filtered_X['Domain'][row_index], 'has been classified as ', prediction, 'and should be ', label
  print(msg_all)

  if int(prediction) != int(label):   # for wrong
    All_Wrong.append(msg_all)
    All.append(msg_all)


df_all_erros = pd.DataFrame  (All_Wrong)  
df_all_predictions = pd.DataFrame  (All)  

y = y.astype(model_predictions.dtype)

################################
# confusion matrix for NN- hash encoding
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
import sklearn.metrics as metrics


print(metrics.classification_report(y, model_predictions))
matrix = confusion_matrix(y, model_predictions)
print(matrix)

def categorize_prediction_level(actual, predicted):
  df = pd.DataFrame()
  df['actual'] = actual
  df['predicted'] = predicted
  df['difference'] = (df['actual'] - df['predicted']).abs()
  return df




np.unique(model_predictions, return_counts=True)
error_level_df = categorize_prediction_level(y, model_predictions)
error_level_df.head()
#######################
#error_level_df['predicted_lr_jain'] = model_predictions_lr_jain
#error_level_df['predicted_dtc_jain'] = model_predictions_dt_jain
#error_level_df['predicted_bags_jain'] = model_predictions_bags_jain
error_level_df['FInalPrediction'] = model_predictions


#Counting errored values
error_level_df['difference'].value_counts()

errored_model_predictions = error_level_df[error_level_df["difference"] >= 1]

errored_model_predictions_1 = error_level_df[ ( (error_level_df["difference"] > 1) & (error_level_df["difference"] <=2) ) ]
errored_model_predictions_2 = error_level_df[ ( (error_level_df["difference"] > 2) & (error_level_df["difference"] <=3) ) ]
errored_model_predictions_3 = error_level_df[ ( (error_level_df["difference"] > 3) & (error_level_df["difference"] <=4) ) ]
errored_model_predictions_4 = error_level_df[ ( (error_level_df["difference"] > 4) & (error_level_df["difference"] <=5) ) ]


total_errored_model_predictions = len(errored_model_predictions)
total_errored_model_predictions_1 = len(errored_model_predictions_1)
total_errored_model_predictions_2 = len(errored_model_predictions_1)
total_errored_model_predictions_3 = len(errored_model_predictions_1)
total_errored_model_predictions_4 = len(errored_model_predictions_1)

print('total errors = ', total_errored_model_predictions )
print('total errors with margin 1 = ', total_errored_model_predictions_1 )
print('total errors with margin 2= ', total_errored_model_predictions_2 )
print('total errors with margin 3 = ', total_errored_model_predictions_3 )
print('total errors with margin 4 = ', total_errored_model_predictions_4 )
print(errored_model_predictions)


# Save in figure
all_lables = ['Error 1','Error 2','Error 3','Error 4','Error 5' ]
all_err = [total_errored_model_predictions, total_errored_model_predictions_1, total_errored_model_predictions_2, total_errored_model_predictions_3, total_errored_model_predictions_4]
plt.bar(all_lables, all_err)
plt.xlabel("Errors' Margin")
plt.ylabel("Total COunt")
plt.title('Counting Total Errors with their deviation', fontsize=18)
plt.savefig(result_path_errors)

"""## ERRORS (MSE, RMSE, AE) & ACCURACY (precision_score, recall_score, f1_score)

---


"""

from sklearn.metrics import mean_squared_error
import math

# Calculate the confusion matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report #  for NN- hash encoding
import io
from io import StringIO
# !pip install xlsxwriter


MSE = mean_squared_error(y, model_predictions)
 
print("Mean Square Error:\n")
print(MSE)

RMSE = math.sqrt(MSE)
print("\nRoot Mean Square Error:\n")
print(RMSE)

acc = accuracy_score(y, model_predictions)
ACCURACY = acc

##################################
# For printing into sheets
##################################

message = "=============Errors============= \n\n\n"
mse_data = "Mean Square Error:        "+ str(MSE) + "\n"
rmse_data = "Root Mean Square Error:  "+ str(RMSE) + "\n"
acc_data = "Accuracy: " + str(acc) + "\n"
all_data = message + mse_data + rmse_data + acc_data 
all_data = all_data + "\n ==============================\n"

df_error = pd.read_csv(io.StringIO(all_data), sep=";")

data_x = ['MSE', 'RMSE']
data_errors = [MSE, RMSE]

fig, axs = plt.subplots(1,2)
fig.suptitle('Errors and Accuracy', fontsize=18)
axs[0].bar(data_x, data_errors)
axs[1].bar('Accuracy', acc)

plt.xlabel("Metrics")
plt.ylabel("Value")
plt.savefig(result_path_accuracy)

###############################



conf_matrix = confusion_matrix(y_true=y, y_pred=model_predictions)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('model_Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()
fig.savefig(result_path_confusion)

# Printing report
report = classification_report(y_true=y, y_pred=model_predictions, output_dict=True)

df_report = pd.DataFrame(report).transpose()
df_confusion = pd.DataFrame(conf_matrix).transpose()

print(df_confusion)
print(df_report)

#######################
#######################################
# Writing results 
#######################################

writer = pd.ExcelWriter(file_to_save_results, engine='xlsxwriter')

df_empty1 = pd.DataFrame()
df_empty2 = pd.DataFrame()
df_empty3 = pd.DataFrame()


# Write data into excel sheets
df_report.to_excel(writer, sheet_name='Accuracy_Reports')
df_error.to_excel(writer, sheet_name='Errors')
# printing errors
#errored_predictions.to_excel(writer, sheet_name='Predictions_with_errors')
df_all_erros.to_excel(writer, sheet_name='Predictions_with_errors_details')
error_level_df.to_excel(writer, sheet_name='Predictions')
df_all_predictions.to_excel(writer, sheet_name='All Predictions')



# Save Accuracy figures in excel file
df_empty1.to_excel(writer, sheet_name='Accuracy_charts')
worksheet_acc = writer.sheets['Accuracy_charts']
worksheet_acc.insert_image('C2',result_path_accuracy)

# Save Error figures in excel file
df_empty2.to_excel(writer, sheet_name='Error_charts')
worksheet_err = writer.sheets['Error_charts']
worksheet_err.insert_image('C2',result_path_errors)


# Save Confusion figures in excel file
df_empty3.to_excel(writer, sheet_name='Confusion')
worksheet_con = writer.sheets['Confusion']
worksheet_con.insert_image('C2',result_path_confusion)


########################################################################################################################################
####################################################################
# What you need to show
####################################################################

#df_all_predictions .... dataframe that is of same lenght as input data
#df_confusion --- its a dataframe 
# df_report  ---its a dataframe
# ACCURACY --- its a value (e.g. 60)

############################################
# If there is any issue reading excel file, e.g. it is corrput etc; pls display this message and dont show any results
os.system("clear")
print("OUTPUT")
if (ERROR_READING == True):
  resultData = {
    "error": 'Failed! Cannot Read Data File; Pls check file'
  }
  print(json.dumps(resultData))
else:
  resultData = {
    "allPredictions": df_all_predictions.to_dict(),
    "confusionMatrix": df_confusion.to_dict(),
    "accuracy": str(ACCURACY),
    "reports": df_report.to_dict(),
    "error": None
  }
  print(json.dumps(resultData))

######################################################################################################################################
#save file
writer.save()